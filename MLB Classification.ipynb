{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Models\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# These models are voting models based off the above models\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# Data prep\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Model evaluations\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import KFold,StratifiedKFold, ShuffleSplit, StratifiedShuffleSplit\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Models\n",
    "\n",
    "svc = SVC(kernel='rbf', gamma=0.1, C=10) # 5% increase with these hyperparamters\n",
    "KNC = KNeighborsClassifier(weights='distance', p=2, n_neighbors=10, metric='euclidean', leaf_size=40) # 2.7% increase with these hp\n",
    "ADBC = AdaBoostClassifier(n_estimators=155, learning_rate=0.8) # 2% increase with these hp\n",
    "RFC = RandomForestClassifier(n_estimators=1000, min_samples_split=5) # 1% better with these hyperparameters\n",
    "\n",
    "GBC = GradientBoostingClassifier(n_estimators=500, learning_rate=0.15) # 2% better\n",
    "HGBC = HistGradientBoostingClassifier(min_samples_leaf=25, max_leaf_nodes=80, max_iter=100, max_depth=None, learning_rate=0.1, l2_regularization=1.5) # 2% better\n",
    "XGB = XGBClassifier(n_estimators=150, learning_rate=0.1) # 1.7% better with hp\n",
    "# QDA = QuadraticDiscriminantAnalysis() # Same with default hp\n",
    "\n",
    "# Imputer\n",
    "imputer = SimpleImputer()\n",
    "MMScaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "svr = SVR()\n",
    "KNC = KNeighborsRegressor()\n",
    "ADBC = AdaBoostRegressor()\n",
    "RFC = RandomForestRegressor()\n",
    "\n",
    "GBC = GradientBoostingRegressor()\n",
    "HGBC = HistGradientBoostingRegressor()\n",
    "XGB = XGBRegressor()\n",
    "\n",
    "# Imputer\n",
    "imputer = SimpleImputer()\n",
    "MMScaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./Scraping/MLB 2020-21.csv', parse_dates=['Date'])\n",
    "data.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "data.sort_values(by=['Date'], inplace=True)\n",
    "\n",
    "# Create y\n",
    "data['Home Points Dif'] = data['Home PTS'] - data['Vis PTS']\n",
    "data['Classification'] = data['Home PTS'] > data['Vis PTS']\n",
    "y_class = data['Classification']\n",
    "\n",
    "# Add dates and time\n",
    "\n",
    "# Get Day, Month and Year from date column\n",
    "dates = pd.DataFrame()\n",
    "dates['Year'] = data['Date'].dt.strftime('%Y')\n",
    "dates['Month'] = data['Date'].dt.strftime('%m')\n",
    "dates['Day'] = data['Date'].dt.strftime('%d')\n",
    "data = pd.concat([data, dates], axis=1)\n",
    "\n",
    "\n",
    "# Result of the teams last game\n",
    "\n",
    "data[\"HomeLastDif\"] = 0\n",
    "data[\"VisitorLastDif\"] = 0\n",
    "\n",
    "from collections import defaultdict\n",
    "won_last = defaultdict(int) # Create dictionary won last\n",
    "\n",
    "for index, row in data.iterrows(): # for each row\n",
    "    home_team = row['Home'] # Take the home team in the row\n",
    "    visitor_team = row['Visitor'] # Take the vis team in each row\n",
    "    row['HomeLastDif'] = won_last[home_team] # If HomeLastWin is true set that team to won in the won_last dict\n",
    "    row['VisitorLastDif'] = won_last[visitor_team] # If VisitorLastWin is true set that team to won in won_last dict\n",
    "    data.loc[index] = row # Set the index for the next row?\n",
    "    # Set current win\n",
    "    won_last[home_team] = row['Home Points Dif'] # If home won set that in the won_last dict\n",
    "    won_last[visitor_team] = - row['Home Points Dif'] # if home did not win set that in the won_last dict\n",
    "    \n",
    "# Add WinStreaks\n",
    "\n",
    "data['HomeWinStreak'] = 0\n",
    "data['VisitorWinStreak'] = 0\n",
    "\n",
    "win_streak = defaultdict(int) #  Create a dictionary for teams winning streaks\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    home_team = row['Home'] # Home team = home team for that row\n",
    "    visitor_team = row['Visitor'] # Vis team = vis team for that row\n",
    "    row['HomeWinStreak'] = win_streak[home_team] # HomeWinStreak for that row is looked up in the dictionary win_streak\n",
    "    row['VisitorWinStreak'] = win_streak[visitor_team] # Set VisitorWinStreak in the row to dict value for that team\n",
    "    data.loc[index] = row # Set row to next row\n",
    "    # Set current win streak number\n",
    "    if row['Home Points Dif']:\n",
    "        win_streak[home_team] += 1\n",
    "        win_streak[visitor_team] = 0\n",
    "    else:\n",
    "        win_streak[home_team] = 0\n",
    "        win_streak[visitor_team] += 1\n",
    "        \n",
    "\n",
    "# Which team won in their last match?\n",
    "\n",
    "last_match_winner = defaultdict(int)\n",
    "\n",
    "def home_team_won_last(row):\n",
    "    # Variables equal the team names\n",
    "    home_team = row['Home']\n",
    "    visitor_team = row['Visitor']\n",
    "\n",
    "    teams = tuple(sorted([home_team, visitor_team])) # Tuple of the home and visitor team to search for\n",
    "    result = 1 if last_match_winner[teams] == row['Home'] else 0 # Look in last_match_winner dict for if these teams have played before\n",
    "    winner = row['Home'] if  row['Home Points Dif'] else row['Visitor'] # Winner variable is home team if the homewin column says it is\n",
    "\n",
    "    last_match_winner[teams] = winner # Feed the winner into the last_match_winner dict\n",
    "\n",
    "    return result\n",
    "\n",
    "data['HomeTeamWonLast'] = data.apply(home_team_won_last, axis=1) # Apply the function on each row (axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:,'Year':]\n",
    "y = data['Home Points Dif']\n",
    "y_class = data['Classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model and accuracy functions\n",
    "\n",
    "def train_model(X_train_and_test, y_train_and_test, model):\n",
    "    ''' Scale, Split, Impute and Train one model '''\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train_and_test, y_train_and_test, test_size=0.2, shuffle=False)\n",
    "\n",
    "    pipe = make_pipeline(SimpleImputer(),StandardScaler(), model)\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict(X_test)\n",
    "\n",
    "    preds_df = pd.DataFrame(preds, columns=['Predictions'])\n",
    "    predictions_array.append(preds_df)\n",
    "    \n",
    "def accuracy():    \n",
    "    past_predictions = ''\n",
    "    _, _, _, y_test = train_test_split(X, y_class, test_size=0.2, shuffle=False)\n",
    "    past_predictions = pd.DataFrame(y_test)\n",
    "    past_predictions = past_predictions.reset_index(drop=True)\n",
    "\n",
    "    for i in predictions_array:\n",
    "        df = pd.DataFrame(i)\n",
    "        df.reset_index(drop=True)\n",
    "        past_predictions = pd.concat([past_predictions, df], axis=1, ignore_index=True)\n",
    "\n",
    "    past_predictions.columns = ['Actual', 'SVC', 'KNC', 'ADBC', 'RFC', 'GBC', 'HGBC', 'XGB']\n",
    "    \n",
    "    for i in past_predictions.columns:\n",
    "        print(i)\n",
    "        true_pos = len(past_predictions[(past_predictions[i] == True) & (past_predictions['Actual'] == True)])\n",
    "        false_pos = len(past_predictions[(past_predictions[i] == True) & (past_predictions['Actual'] == False)])\n",
    "        true_neg = len(past_predictions[(past_predictions[i] == False) & (past_predictions['Actual'] == False)])\n",
    "        false_neg = len(past_predictions[(past_predictions[i] == False) & (past_predictions['Actual'] == True)])\n",
    "\n",
    "        print('True Pos: ', true_pos, ' / ', (past_predictions[i]==True).sum())\n",
    "        print('Win Acc: ', true_pos/(true_pos+false_pos))\n",
    "        print('True Neg: ', true_neg, ' / ', (past_predictions[i]==False).sum())\n",
    "        print('Lose Acc: ', true_neg/(true_neg+false_neg))\n",
    "        print('Total Acc: ', (true_pos + true_neg)/len(past_predictions))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_no_year = X.loc[:,'HomeLastDif':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:42:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Actual\n",
      "True Pos:  175  /  175\n",
      "Win Acc:  1.0\n",
      "True Neg:  174  /  174\n",
      "Lose Acc:  1.0\n",
      "Total Acc:  1.0\n",
      "\n",
      "SVC\n",
      "True Pos:  51  /  87\n",
      "Win Acc:  0.5862068965517241\n",
      "True Neg:  138  /  262\n",
      "Lose Acc:  0.5267175572519084\n",
      "Total Acc:  0.5415472779369628\n",
      "\n",
      "KNC\n",
      "True Pos:  68  /  125\n",
      "Win Acc:  0.544\n",
      "True Neg:  117  /  224\n",
      "Lose Acc:  0.5223214285714286\n",
      "Total Acc:  0.5300859598853869\n",
      "\n",
      "ADBC\n",
      "True Pos:  82  /  158\n",
      "Win Acc:  0.5189873417721519\n",
      "True Neg:  98  /  191\n",
      "Lose Acc:  0.5130890052356021\n",
      "Total Acc:  0.5157593123209169\n",
      "\n",
      "RFC\n",
      "True Pos:  45  /  94\n",
      "Win Acc:  0.4787234042553192\n",
      "True Neg:  125  /  255\n",
      "Lose Acc:  0.49019607843137253\n",
      "Total Acc:  0.4871060171919771\n",
      "\n",
      "GBC\n",
      "True Pos:  26  /  58\n",
      "Win Acc:  0.4482758620689655\n",
      "True Neg:  142  /  291\n",
      "Lose Acc:  0.4879725085910653\n",
      "Total Acc:  0.4813753581661891\n",
      "\n",
      "HGBC\n",
      "True Pos:  55  /  117\n",
      "Win Acc:  0.4700854700854701\n",
      "True Neg:  112  /  232\n",
      "Lose Acc:  0.4827586206896552\n",
      "Total Acc:  0.4785100286532951\n",
      "\n",
      "XGB\n",
      "True Pos:  28  /  72\n",
      "Win Acc:  0.3888888888888889\n",
      "True Neg:  130  /  277\n",
      "Lose Acc:  0.4693140794223827\n",
      "Total Acc:  0.45272206303724927\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models_array = [svc, KNC, ADBC, RFC, GBC, HGBC, XGB]\n",
    "\n",
    "\n",
    "predictions_array = []\n",
    "\n",
    "for model in models_array:\n",
    "    train_model(X, y_class, model)\n",
    "accuracy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bet on home baseline:  0.527283170591614\n"
     ]
    }
   ],
   "source": [
    "print('Bet on home baseline: ', len(data[data['Classification'] == True]) / len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
